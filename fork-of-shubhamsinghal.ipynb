{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":86142,"databundleVersionId":9786425,"sourceType":"competition"},{"sourceId":9607708,"sourceType":"datasetVersion","datasetId":5862129}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\nfrom PIL import Image\n\nimport tensorflow as tf\n\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_folder='/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/train'\n\ntrain_path = []\n\n\n\nfor root, dirs, files in os.walk(train_folder):\n\n    for file in files:\n\n            train_path.append(os.path.join(root, file))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask_path = []\n\n\n\nfor image_path in train_path:\n\n    mask_file = image_path.replace('leftImg8bit.jpg', 'gtFine_labelColors.png')  \n\n    mask_file =mask_file.replace('train', 'labels') \n\n    mask_path.append(mask_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"json_path = []\n\n\n\nfor image_path in train_path:\n\n    json_file = image_path.replace('leftImg8bit.jpg', 'gtFine_polygons.json')  \n\n    json_file =json_file.replace('train', 'labels') \n\n    json_path.append(json_file)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_list=[(train_path[i], mask_path[i], json_path[i]) for i in range(len(train_path))]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We know parking, animal, tunnel, traffic light and rail track are in minority","metadata":{}},{"cell_type":"code","source":"pip install shapely","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nfrom shapely.geometry import Polygon\n\n\n\nimage_width = 1920\n\nimage_length = 1080\n\narea = image_width * image_length\n\n\n\ndef calculate_polygon_area(polygon_coords):\n\n    polygon = Polygon(polygon_coords)\n\n    return polygon.area\n\n\n\n\n\ndef get_per(json_file, label_min):\n\n    with open(json_file, 'r') as f:\n\n        data = json.load(f)\n\n\n\n    label_area = 0\n\n    found_polygon = False  # Flag to check if any polygon was found\n\n\n\n    for obj in data['objects']:\n\n        if obj['label'] == label_min:\n\n            found_polygon = True  # Set the flag to True when a matching polygon is found\n\n            polygon_coords = obj.get('polygon', None)  # Use get to avoid KeyError\n\n            \n\n            # Check if polygon_coords is None or has insufficient coordinates\n\n            if polygon_coords is None or len(polygon_coords) < 4:\n\n                print(f\"Skipping polygon with insufficient coordinates in file '{json_file}': {polygon_coords}\")  # Debugging line\n\n                continue  # Skip this polygon if it doesn't have enough coordinates\n\n            \n\n            # Calculate the area if the polygon is valid\n\n            label_area += calculate_polygon_area(polygon_coords)\n\n\n\n    # If no valid polygons were found, handle accordingly\n\n    if not found_polygon:\n\n        \n\n        return 0  # Return 0% if no polygons were found\n\n\n\n    label_percentage = (label_area / area) * 100 if area > 0 else 0  # Avoid division by zero\n\n    return label_percentage\n\n\n\n\n\n\n\n\n\n\n\ndef top(combined_list, label_min, top_n):\n\n    label_percentages = []\n\n\n\n    for entry in combined_list:\n\n        json_file = entry[2]\n\n        label_percentage = get_per(json_file, label_min)\n\n        label_percentages.append((label_percentage, entry))\n\n\n\n    label_percentages.sort(reverse=True, key=lambda x: x[0])\n\n    top_files = label_percentages[:top_n]\n\n    \n\n    return top_files\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:39:49.177434Z","iopub.execute_input":"2024-10-12T16:39:49.177815Z","iopub.status.idle":"2024-10-12T16:39:49.191453Z","shell.execute_reply.started":"2024-10-12T16:39:49.177776Z","shell.execute_reply":"2024-10-12T16:39:49.190527Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"parking\" \n\ntop_files_parking = top(combined_list, label_name, top_n=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:46:40.963308Z","iopub.execute_input":"2024-10-12T16:46:40.96374Z","iopub.status.idle":"2024-10-12T16:47:28.709387Z","shell.execute_reply.started":"2024-10-12T16:46:40.963699Z","shell.execute_reply":"2024-10-12T16:47:28.708524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"animal\" \n\ntop_files = top(combined_list, label_name, top_n=100)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:42:02.64882Z","iopub.execute_input":"2024-10-12T16:42:02.649221Z","iopub.status.idle":"2024-10-12T16:43:35.124329Z","shell.execute_reply.started":"2024-10-12T16:42:02.64918Z","shell.execute_reply":"2024-10-12T16:43:35.123274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"traffic light\" \n\ntop_files_tl= top(combined_list, label_name, top_n=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:43:35.126161Z","iopub.execute_input":"2024-10-12T16:43:35.126464Z","iopub.status.idle":"2024-10-12T16:44:23.657419Z","shell.execute_reply.started":"2024-10-12T16:43:35.126431Z","shell.execute_reply":"2024-10-12T16:44:23.656372Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"tunnel\" \n\ntop_files_tunnel = top(combined_list, label_name, top_n=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:44:23.659Z","iopub.execute_input":"2024-10-12T16:44:23.659324Z","iopub.status.idle":"2024-10-12T16:45:11.44026Z","shell.execute_reply.started":"2024-10-12T16:44:23.659283Z","shell.execute_reply":"2024-10-12T16:45:11.439178Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_name = \"rail track\" \n\ntop_files_rail_track = top(combined_list, label_name, top_n=100)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:45:11.442571Z","iopub.execute_input":"2024-10-12T16:45:11.442904Z","iopub.status.idle":"2024-10-12T16:45:59.997076Z","shell.execute_reply.started":"2024-10-12T16:45:11.442868Z","shell.execute_reply":"2024-10-12T16:45:59.996226Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"min_list  = top_files_rail_track[:12] + top_files_tunnel[:5] +top_files_parking[:20] +top_files[:50] + top_files_parking[:50]","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:47:28.711003Z","iopub.execute_input":"2024-10-12T16:47:28.711366Z","iopub.status.idle":"2024-10-12T16:47:28.717531Z","shell.execute_reply.started":"2024-10-12T16:47:28.71133Z","shell.execute_reply":"2024-10-12T16:47:28.716421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nsmall_df = random.sample(combined_list, 500)\n\n\n\nmin_df = [entry[1] for entry in min_list]\n\n\n\nfinal_df= small_df+min_df","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:47:28.718791Z","iopub.execute_input":"2024-10-12T16:47:28.719058Z","iopub.status.idle":"2024-10-12T16:47:28.729182Z","shell.execute_reply.started":"2024-10-12T16:47:28.719027Z","shell.execute_reply":"2024-10-12T16:47:28.728272Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We will convert 3d tensor of rgb segmented images to 1d tensor storing label of that pixel","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfinal_df=pd.DataFrame(final_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:46:00.053456Z","iopub.status.idle":"2024-10-12T16:46:00.053833Z","shell.execute_reply.started":"2024-10-12T16:46:00.053658Z","shell.execute_reply":"2024-10-12T16:46:00.053677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df=final_df.rename(columns={0:\"image\", 1:\"seg\",2:\"json\"})","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:46:00.055706Z","iopub.status.idle":"2024-10-12T16:46:00.05638Z","shell.execute_reply.started":"2024-10-12T16:46:00.056103Z","shell.execute_reply":"2024-10-12T16:46:00.056134Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport cv2\n\nimport numpy as np\n\nimport pandas as pd\n\nimport json\n\nimport os\n\n\n\n\n\nclass Label:\n\n    def __init__(self, name, id, csId, csTrainId, level4id, level3Id, category, level2Id, level1Id, hasInstances, ignoreInEval, color):\n\n        self.name = name\n\n        self.id = id\n\n        self.csId = csId\n\n        self.csTrainId = csTrainId\n\n        self.level4id = level4id\n\n        self.level3Id = level3Id\n\n        self.category = category\n\n        self.level2Id = level2Id\n\n        self.level1Id = level1Id\n\n        self.hasInstances = hasInstances\n\n        self.ignoreInEval = ignoreInEval\n\n        self.color = color\n\n\n\n# Your label definitions here (from your provided list)\n\n\n\nlabels_details = [\n\n    #       name                     id    csId     csTrainId level4id        level3Id  category           level2Id      level1Id  hasInstances   ignoreInEval   color\n\n    Label(  'road'   \n          ,  0   ,  7 ,     0 ,       0   ,     0  ,   'drivable'            , 0           , 0      , False        , False        , (128, 64,128)  ),\n\n    Label(  'parking'              ,  1   ,  9 ,   255 ,       1   ,     1  ,   'drivable'            , 1           , 0      , False        , False         , (250,170,160)  ),\n\n    Label(  'drivable fallback'    ,  2   ,  255 ,   255 ,     2   ,       1  ,   'drivable'            , 1           , 0      , False        , False         , ( 81,  0, 81)  ),\n\n    Label(  'sidewalk'             ,  3   ,  8 ,     1 ,       3   ,     2  ,   'non-drivable'        , 2           , 1      , False        , False        , (244, 35,232)  ),\n\n    Label(  'rail track'           ,  4   , 10 ,   255 ,       3   ,     3  ,   'non-drivable'        , 3           , 1      , False        , False         , (230,150,140)  ),\n\n    Label(  'non-drivable fallback',  5   , 255 ,     9 ,      4   ,      3  ,   'non-drivable'        , 3           , 1      , False        , False        , (152,251,152)  ),\n\n    Label(  'person'               ,  6   , 24 ,    11 ,       5   ,     4  ,   'living-thing'        , 4           , 2      , True         , False        , (220, 20, 60)  ),\n\n    Label(  'animal'               ,  7   , 255 ,   255 ,      6   ,      4  ,   'living-thing'        , 4           , 2      , True         , True        , (246, 198, 145)),\n\n    Label(  'rider'                ,  8   , 25 ,    12 ,       7   ,     5  ,   'living-thing'        , 5           , 2      , True         , False        , (255,  0,  0)  ),\n\n    Label(  'motorcycle'           ,  9   , 32 ,    17 ,       8   ,     6  ,   '2-wheeler'           , 6           , 3      , True         , False        , (  0,  0,230)  ),\n\n    Label(  'bicycle'              , 10   , 33 ,    18 ,       9   ,     7  ,   '2-wheeler'           , 6           , 3      , True         , False        , (119, 11, 32)  ),\n\n    Label(  'autorickshaw'         , 11   , 255 ,   255 ,     10   ,      8  ,   'autorickshaw'        , 7           , 3      , True         , False        , (255, 204, 54) ),\n\n    Label(  'car'                  , 12   , 26 ,    13 ,      11   ,     9  ,   'car'                 , 7           , 3      , True         , False        , (  0,  0,142)  ),\n\n    Label(  'truck'                , 13   , 27 ,    14 ,      12   ,     10 ,   'large-vehicle'       , 8           , 3      , True         , False        , (  0,  0, 70)  ),\n\n    Label(  'bus'                  , 14   , 28 ,    15 ,      13   ,     11 ,   'large-vehicle'       , 8           , 3      , True         , False        , (  0, 60,100)  ),\n\n    Label(  'caravan'              , 15   , 29 ,   255 ,      14   ,     12 ,   'large-vehicle'       , 8           , 3      , True         , True         , (  0,  0, 90)  ),\n\n    Label(  'trailer'              , 16   , 30 ,   255 ,      15   ,     12 ,   'large-vehicle'       , 8           , 3      , True         , True         , (  0,  0,110)  ),\n\n    Label(  'train'                , 17   , 31 ,    16 ,      15   ,     12 ,   'large-vehicle'       , 8           , 3      , True         , True        , (  0, 80,100)  ),\n\n    Label(  'vehicle fallback'     , 18   , 355 ,   255 ,     15   ,      12 ,   'large-vehicle'       , 8           , 3      , True         , False        , (136, 143, 153)),\n\n    Label(  'curb'                 , 19   ,255 ,   255 ,      16   ,     13 ,   'barrier'             , 9           , 4      , False        , False        , (220, 190, 40)),\n\n    Label(  'wall'                 , 20   , 12 ,     3 ,      17   ,     14 ,   'barrier'             , 9           , 4      , False        , False        , (102,102,156)  ),\n\n    Label(  'fence'                , 21   , 13 ,     4 ,      18   ,     15 ,   'barrier'             , 10           , 4      , False        , False        , (190,153,153)  ),\n\n    Label(  'guard rail'           , 22   , 14 ,   255 ,      19   ,     16 ,   'barrier'             , 10          , 4      , False        , False         , (180,165,180)  ),\n\n    Label(  'billboard'            , 23   , 255 ,   255 ,     20   ,      17 ,   'structures'          , 11           , 4      , False        , False        , (174, 64, 67) ),\n\n    Label(  'traffic sign'         , 24   , 20 ,     7 ,      21   ,     18 ,   'structures'          , 11          , 4      , False        , False        , (220,220,  0)  ),\n\n    Label(  'traffic light'        , 25   , 19 ,     6 ,      22   ,     19 ,   'structures'          , 11          , 4      , False        , False        , (250,170, 30)  ),\n\n    Label(  'pole'                 , 26   , 17 ,     5 ,      23   ,     20 ,   'structures'          , 12          , 4      , False        , False        , (153,153,153)  ),\n\n    Label(  'polegroup'            , 27   , 18 ,   255 ,      23   ,     20 ,   'structures'          , 12          , 4      , False        , False         , (153,153,153)  ),\n\n    Label(  'obs-str-bar-fallback' , 28   , 255 ,   255 ,     24   ,      21 ,   'structures'          , 12          , 4      , False        , False        , (169, 187, 214) ),\n\n    Label(  'building'             , 29   , 11 ,     2 ,      25   ,     22 ,   'construction'        , 13          , 5      , False        , False        , ( 70, 70, 70)  ),\n\n    Label(  'bridge'               , 30   , 15 ,   255 ,      26   ,     23 ,   'construction'        , 13          , 5      , False        , False         , (150,100,100)  ),\n\n    Label(  'tunnel'               , 31   , 16 ,   255 ,      26   ,     23 ,   'construction'        , 13          , 5      , False        , False         , (150,120, 90)  ),\n\n    Label(  'vegetation'           , 32   , 21 ,     8 ,      27   ,     24 ,   'vegetation'          , 14          , 5      , False        , False        , (107,142, 35)  ),\n\n    Label(  'sky'                  , 33   , 23 ,    10 ,      28   ,     25 ,   'sky'                 , 15          , 6      , False        , False        , ( 70,130,180)  ),\n\n    Label(  'fallback background'  , 34   , 255 ,   255 ,     29   ,      25 ,   'object fallback'     , 15          , 6      , False        , False        , (169, 187, 214)),\n\n    Label(  'unlabeled'            , 35   ,  0  ,     255 ,   255   ,      255 ,   'void'                , 255         , 255    , False        , True         , (  0,  0,  0)  ),\n\n    Label(  'ego vehicle'          , 36   ,  1  ,     255 ,   255   ,      255 ,   'void'                , 255         , 255    , False        , True         , (  0,  0,  0)  ),\n\n    Label(  'rectification border' , 37   ,  2  ,     255 ,   255   ,      255 ,   'void'                , 255         , 255    , False        , True         , (  0,  0,  0)  ),\n\n    Label(  'out of roi'           , 38   ,  3  ,     255 ,   255   ,      255 ,   'void'                , 255         , 255    , False        , True         , (  0,  0,  0)  ),\n\n    Label(  'license plate'        , 39   , 255 ,     255 ,   255   ,      255 ,   'vehicle'             , 255         , 255    , False        , True         , (  0,  0,142)  ),\n\n\n\n]\n\n\n\n\n\n\n\n# Function to get label information from pixel color\n\ndef get_label_by_color(color):\n\n    for label in labels_details:\n\n        if label.color == tuple(color):  # Compare the pixel color to the label color\n\n            return label\n\n    return None\n\n\n\nimport torch\n\ndef get_id_by_color(color_tuple):\n\n    # Convert the input color to a torch tensor on CPU\n\n    input_color = torch.tensor(color_tuple, dtype=torch.float32)  # No .cuda() here\n\n\n\n    # Extract all label colors and convert them to a tensor on CPU\n\n    label_colors = torch.stack([torch.tensor(label.color, dtype=torch.float32) for label in labels_details])  # No .cuda() here\n\n\n\n    # Compute the squared differences (to avoid sqrt for performance)\n\n    distances = torch.sum((label_colors - input_color) ** 2, dim=1)  # Shape: [num_labels]\n\n\n\n    # Find the index of the minimum distance\n\n    closest_index = torch.argmin(distances).item()  # Get the index of the closest label\n\n\n\n    return labels_details[closest_index].id\n\n\n\ndef id_to_label_map():\n\n    new_dict = {}\n\n    for label in labels_details:\n\n        new_dict[int(label.id)] = label.name\n\n    return new_dict","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:46:00.058157Z","iopub.status.idle":"2024-10-12T16:46:00.05854Z","shell.execute_reply.started":"2024-10-12T16:46:00.058339Z","shell.execute_reply":"2024-10-12T16:46:00.058357Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"i= Image.open(final_df['seg'][0])\n\nplt.imshow(i)\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:46:00.059971Z","iopub.status.idle":"2024-10-12T16:46:00.060329Z","shell.execute_reply.started":"2024-10-12T16:46:00.060143Z","shell.execute_reply":"2024-10-12T16:46:00.060161Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\nfrom PIL import Image\n\nimport torch\n\n\n\n\n\ndef convert_id(image):\n\n    # Convert image to RGB and resize to (512, 512)\n\n    image_array = np.array(image.convert('RGB').resize((512, 512)))\n\n    # Create an empty tensor for ID labels\n\n    id_array = torch.zeros((image_array.shape[0], image_array.shape[1]), dtype=torch.uint8)  # No .cuda() here\n\n    unique_colors, indices = torch.unique(torch.tensor(image_array).view(-1, 3), dim=0, return_inverse=True)\n\n\n\n    # Replace this with your actual color-to-ID mapping function\n\n    label_ids = torch.tensor([get_id_by_color(tuple(color.tolist())) for color in unique_colors])  # No .cuda() here\n\n    id_array = label_ids[indices]  # Map colors to IDs\n\n\n\n    return id_array\n\n\n\n\n\ndata_set = []\n\n\n\n# Process images directly from the provided lists\n\nfor index, row in tqdm(final_df.iterrows(), desc=\"Processing images\", total=final_df.shape[0]):\n\n    train_img_path = row['image']\n\n    label_img_path = row['seg']\n\n    # Load the train image and resize\n\n    pixel_image = Image.open(train_img_path).resize((512, 512))  # Train image\n\n\n\n    # Load the label image, process it, and transform it to a tensor\n\n    label_image_tensor = convert_id(Image.open(label_img_path))  # Label image transformed to label IDs\n\n\n\n    # Add the image pair to the dataset\n\n    data_set.append({\n\n        'pixels': pixel_image,  # Train image as PIL Image\n\n        'labels': label_image_tensor  # Label image as torch tensor on CPU\n\n    })\n\n\n\n# Example to show the contents of data_set\n\nfor idx, data in enumerate(data_set[:3]):  # Show first 3 entries for illustration\n\n    print(f\"Index {idx}:\")\n\n    print(f\"Pixels Image: {data['pixels']}\")\n\n    print(f\"Labels Image (Tensor): {data['labels']}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:40:28.244253Z","iopub.status.idle":"2024-10-12T16:40:28.244712Z","shell.execute_reply.started":"2024-10-12T16:40:28.2445Z","shell.execute_reply":"2024-10-12T16:40:28.244529Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(data_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T16:40:28.246299Z","iopub.status.idle":"2024-10-12T16:40:28.246874Z","shell.execute_reply.started":"2024-10-12T16:40:28.246574Z","shell.execute_reply":"2024-10-12T16:40:28.24661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Load the pkl file\nwith open('/kaggle/input/tensor-vlaues/data_s.pkl', 'rb') as file:\n    data_set = pickle.load(file)\n\n# Print or inspect the data\nprint(data_set)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T02:55:57.504659Z","iopub.execute_input":"2024-10-13T02:55:57.504964Z","iopub.status.idle":"2024-10-13T02:56:21.282578Z","shell.execute_reply.started":"2024-10-13T02:55:57.504927Z","shell.execute_reply":"2024-10-13T02:56:21.281539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\n\n\n# Iterate through each item in the data_set\n\nfor entry in data_set:\n\n    # Reshape the labels tensor to (512, 512)\n\n    entry['labels'] = entry['labels'].reshape(512, 512)\n\n\n\n# Example to show the shapes of updated labels\n\nfor idx, data in enumerate(data_set[:3]):  # Show first 3 entries for illustration\n\n    print(f\"Index {idx}:\")\n\n    print(f\"Pixels Image: {data['pixels']}\")\n\n    print(f\"Labels Image Shape: {data['labels'].shape}\")  # Should be (512, 512)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:41:02.526193Z","iopub.execute_input":"2024-10-12T16:41:02.527055Z","iopub.status.idle":"2024-10-12T16:41:02.541898Z","shell.execute_reply.started":"2024-10-12T16:41:02.527008Z","shell.execute_reply":"2024-10-12T16:41:02.540976Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Assuming you have a PIL image object called 'image'\n\ndef visualize_pil_image(image):\n\n    plt.imshow(image)\n\n    plt.axis('off')  # Hide axis\n\n    plt.show()\n\n\n\n# Example usage: Assuming 'data_set' has PIL images in 'pixels'\n\n# Visualize the first image in data_set\n\nvisualize_pil_image(data_set[0]['pixels'])\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:56:21.284327Z","iopub.execute_input":"2024-10-13T02:56:21.285185Z","iopub.status.idle":"2024-10-13T02:56:21.549347Z","shell.execute_reply.started":"2024-10-13T02:56:21.285137Z","shell.execute_reply":"2024-10-13T02:56:21.548467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Transfer learning on SegTransformer\n","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:40:28.256395Z","iopub.status.idle":"2024-10-12T16:40:28.256924Z","shell.execute_reply.started":"2024-10-12T16:40:28.256652Z","shell.execute_reply":"2024-10-12T16:40:28.256679Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-12T10:26:36.893902Z","iopub.status.idle":"2024-10-12T10:26:36.894226Z","shell.execute_reply.started":"2024-10-12T10:26:36.89406Z","shell.execute_reply":"2024-10-12T10:26:36.894077Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preparing the dataset","metadata":{}},{"cell_type":"code","source":"id2label = id_to_label_map()\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T16:41:22.534389Z","iopub.execute_input":"2024-10-12T16:41:22.534804Z","iopub.status.idle":"2024-10-12T16:41:22.54187Z","shell.execute_reply.started":"2024-10-12T16:41:22.534767Z","shell.execute_reply":"2024-10-12T16:41:22.540763Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-10-12T10:26:36.901152Z","iopub.status.idle":"2024-10-12T10:26:36.901484Z","shell.execute_reply.started":"2024-10-12T10:26:36.901315Z","shell.execute_reply":"2024-10-12T10:26:36.901332Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-12T10:26:36.902534Z","iopub.status.idle":"2024-10-12T10:26:36.902914Z","shell.execute_reply.started":"2024-10-12T10:26:36.902731Z","shell.execute_reply":"2024-10-12T10:26:36.902751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --user albumentations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T10:26:36.90464Z","iopub.status.idle":"2024-10-12T10:26:36.90521Z","shell.execute_reply.started":"2024-10-12T10:26:36.904924Z","shell.execute_reply":"2024-10-12T10:26:36.904954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\n\n\n# Assuming data_set is your list of dictionaries\n\nrandom.seed(1)  # Set seed for reproducibility\n\nrandom.shuffle(data_set)  # Shuffle the list in place\n\n\n\n# Define the split ratios\n\ntest_size = 0.2  # 20% for test\n\nval_size = 0.05   # 5% for validation from the training set\n\n\n\n# Calculate the split indices\n\nsplit_index_test = int(len(data_set) * (1 - test_size))  # Index for test set\n\ntrain_images = data_set[:split_index_test]  # Training data\n\ntest_images = data_set[split_index_test:]    # Testing data\n\n\n\n# Further split the training data for validation\n\nsplit_index_val = int(len(train_images) * (1 - val_size))  # Index for validation set\n\ntrain_images, val_images = train_images[:split_index_val], train_images[split_index_val:]  # Final training and validation data\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:56:52.820299Z","iopub.execute_input":"2024-10-13T02:56:52.820919Z","iopub.status.idle":"2024-10-13T02:56:52.828118Z","shell.execute_reply.started":"2024-10-13T02:56:52.820879Z","shell.execute_reply":"2024-10-13T02:56:52.827183Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nimport numpy as np\n\nfrom torchvision import transforms as T\n\nfrom torch.utils.data import Dataset\n\nfrom PIL import Image\n\nimport torch\n\nimport albumentations as A\n\n\n\nclass SemanticSegmentationDataset(Dataset):\n\n    def __init__(self, data_set, transform=None):\n\n        \"\"\"\n\n        Dataset for Semantic Segmentation.\n\n\n\n        Args:\n\n          - data_set: List of dictionaries, each containing 'pixels' (PIL Image) and 'labels' (Tensor).\n\n          - transform: Transformations to apply to the images and masks.\n\n        \"\"\"\n\n        self.data_set = data_set  # List of dictionaries with 'pixels' and 'labels'\n\n        self.transform = transform\n\n\n\n    def __len__(self):\n\n        return len(self.data_set)\n\n\n\n    def __getitem__(self, idx):\n\n        # Load the image and mask from the data_set\n\n        image = self.data_set[idx]['pixels']\n\n        mask = self.data_set[idx]['labels']\n\n\n\n        # Convert to numpy arrays for transformations\n\n        original_image = np.array(image)\n\n        original_segmentation_map = np.array(mask)\n\n\n\n        # Apply transformations if any\n\n        if self.transform:\n\n            transformed = self.transform(image=original_image, mask=original_segmentation_map)\n\n            transformed_image = transformed['image']  # Transformed image\n\n            transformed_segmentation_map = transformed['mask']  # Transformed mask\n\n        else:\n\n            # If no transformation is applied, return original images and masks\n\n            transformed_image = original_image\n\n            transformed_segmentation_map = original_segmentation_map\n\n\n\n        # Convert the image to a tensor and rearrange the dimensions\n\n        transformed_image = T.ToTensor()(transformed_image).float()  # Convert to tensor\n\n        transformed_segmentation_map = torch.tensor(transformed_segmentation_map, dtype=torch.long)  # Convert to tensor\n\n\n\n        return {\n\n            'pixel_values': transformed_image,  # Image tensor\n\n            'labels': transformed_segmentation_map  # Mask tensor\n\n        }\n\n\n\n# Define your transformations without A.pytorch\n\ntrain_transforms = A.Compose([\n\n    A.RandomCrop(width=512, height=512),\n\n    A.HorizontalFlip(p=0.5),\n\n    A.Normalize(mean=[0.485, 0.456, 0.406],\n\n                std=[0.229, 0.224, 0.225]),\n\n])\n\n\n\nval_transforms = A.Compose([\n\n    A.Resize(width=512, height=512),\n\n\n    A.Normalize(mean=[0.485, 0.456, 0.406],\n\n                std=[0.229, 0.224, 0.225]),\n\n])\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:56:54.435604Z","iopub.execute_input":"2024-10-13T02:56:54.435954Z","iopub.status.idle":"2024-10-13T02:56:59.880656Z","shell.execute_reply.started":"2024-10-13T02:56:54.435921Z","shell.execute_reply":"2024-10-13T02:56:59.879637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Length of training dataset:\", len(data_set))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T02:56:59.882041Z","iopub.execute_input":"2024-10-13T02:56:59.882432Z","iopub.status.idle":"2024-10-13T02:56:59.886787Z","shell.execute_reply.started":"2024-10-13T02:56:59.882397Z","shell.execute_reply":"2024-10-13T02:56:59.885966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create datasets with the appropriate transformations applied\n\ntrain_ds = SemanticSegmentationDataset(train_images, transform=train_transforms)\n\nval_ds = SemanticSegmentationDataset(val_images, transform=val_transforms)\n\ntest_ds = SemanticSegmentationDataset(test_images, transform=val_transforms)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T02:56:59.888066Z","iopub.execute_input":"2024-10-13T02:56:59.888429Z","iopub.status.idle":"2024-10-13T02:56:59.903605Z","shell.execute_reply.started":"2024-10-13T02:56:59.888392Z","shell.execute_reply":"2024-10-13T02:56:59.902866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Check if the datasets return properly formatted samples\n\nsample = train_ds[0]  # Get the first sample from the training dataset\n\nprint(sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T02:57:01.112794Z","iopub.execute_input":"2024-10-13T02:57:01.113569Z","iopub.status.idle":"2024-10-13T02:57:01.256572Z","shell.execute_reply.started":"2024-10-13T02:57:01.113529Z","shell.execute_reply":"2024-10-13T02:57:01.255631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\n# Convert tensor to numpy array for visualization\n\nimage = sample['pixel_values'].permute(1, 2, 0).numpy()  # Change from (C, H, W) to (H, W, C)\n\nmask = sample['labels'].numpy()\n\n\n\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\n\nplt.imshow(image)\n\nplt.title(\"Transformed Image\")\n\n\n\nplt.subplot(1, 2, 2)\n\nplt.imshow(mask, cmap='gray')\n\nplt.title(\"Segmentation Mask\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T02:57:03.194905Z","iopub.execute_input":"2024-10-13T02:57:03.195671Z","iopub.status.idle":"2024-10-13T02:57:03.721827Z","shell.execute_reply.started":"2024-10-13T02:57:03.195624Z","shell.execute_reply":"2024-10-13T02:57:03.720918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_ds[1]['labels'].shape","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:57:18.466182Z","iopub.execute_input":"2024-10-13T02:57:18.466543Z","iopub.status.idle":"2024-10-13T02:57:18.481926Z","shell.execute_reply.started":"2024-10-13T02:57:18.466509Z","shell.execute_reply":"2024-10-13T02:57:18.480902Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the number of samples in the dataset\n\nnum_samples = len(train_ds)\n\nprint(\"Number of samples in the training dataset:\", num_samples)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:57:19.498271Z","iopub.execute_input":"2024-10-13T02:57:19.499174Z","iopub.status.idle":"2024-10-13T02:57:19.503881Z","shell.execute_reply.started":"2024-10-13T02:57:19.49913Z","shell.execute_reply":"2024-10-13T02:57:19.502895Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if the datasets return properly formatted samples\n\nsample = train_ds[0]  # Get the first sample from the training dataset\n\nprint(\"Pixel Values Shape:\", sample['pixel_values'].shape)\n\nprint(\"Labels Shape:\", sample['labels'].shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T02:57:20.270437Z","iopub.execute_input":"2024-10-13T02:57:20.270781Z","iopub.status.idle":"2024-10-13T02:57:20.285699Z","shell.execute_reply.started":"2024-10-13T02:57:20.270744Z","shell.execute_reply":"2024-10-13T02:57:20.284495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n\nfrom PIL import Image\n\nimport requests\n\n\n\nprocessor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n\nmodel = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T03:00:06.712778Z","iopub.execute_input":"2024-10-13T03:00:06.713174Z","iopub.status.idle":"2024-10-13T03:00:07.032069Z","shell.execute_reply.started":"2024-10-13T03:00:06.713115Z","shell.execute_reply":"2024-10-13T03:00:07.031318Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\n\n\n\nepochs = 50\n\nlr = 0.00006\n\nbatch_size = 2\n\n\n\nhub_model_id = \"segformer-b0-finetuned-segments-sidewalk-2\"\n\n\n\ntraining_args = TrainingArguments(\n\n    \"segformer-b0-finetuned-segments-sidewalk-outputs\",\n\n    learning_rate=lr,\n\n    num_train_epochs=epochs,\n\n    per_device_train_batch_size=batch_size,\n\n    per_device_eval_batch_size=batch_size,\n\n    save_total_limit=3,\n\n    eval_strategy=\"steps\",\n\n    save_strategy=\"steps\",\n\n    save_steps=20,\n\n    eval_steps=20,\n\n    logging_steps=1,\n\n    eval_accumulation_steps=5,\n\n    load_best_model_at_end=True,\n\n    push_to_hub=True,\n\n    hub_model_id=hub_model_id,\n\n    hub_strategy=\"end\",\n\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T03:00:07.062478Z","iopub.execute_input":"2024-10-13T03:00:07.062774Z","iopub.status.idle":"2024-10-13T03:00:07.098633Z","shell.execute_reply.started":"2024-10-13T03:00:07.062742Z","shell.execute_reply":"2024-10-13T03:00:07.097918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace this with your actual Hugging Face token\nhugging_face_token = \"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\"\n\n# Log in to Hugging Face\nlogin(token=hugging_face_token)\n\nprint(\"Successfully logged in to Hugging Face.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:00:07.27742Z","iopub.execute_input":"2024-10-13T03:00:07.277687Z","iopub.status.idle":"2024-10-13T03:00:07.376014Z","shell.execute_reply.started":"2024-10-13T03:00:07.277657Z","shell.execute_reply":"2024-10-13T03:00:07.375194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, default_data_collator\n\ntrainer = Trainer(\n\n    model=model,\n\n    args=training_args,\n\n    train_dataset=train_ds,\n\n    eval_dataset=val_ds,\n\n    tokenizer=processor,  # If needed\n\n    data_collator=default_data_collator,  # Ensure you have the correct data collator\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T03:00:08.761737Z","iopub.execute_input":"2024-10-13T03:00:08.762093Z","iopub.status.idle":"2024-10-13T03:00:08.870001Z","shell.execute_reply.started":"2024-10-13T03:00:08.762061Z","shell.execute_reply":"2024-10-13T03:00:08.869116Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:00:09.748662Z","iopub.execute_input":"2024-10-13T03:00:09.749025Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/segformer-b0-finetuned-segments-sidewalk-outputs/checkpoint-3360/rng_state.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport albumentations as A\n\n# Define the validation transformations\nval_transforms = A.Compose([\n    A.Resize(width=512, height=512),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef create_color_map(segmentation_mask, labels):\n    \"\"\"Creates a color map from the segmentation mask using provided labels.\"\"\"\n    color_map = np.zeros((segmentation_mask.shape[0], segmentation_mask.shape[1], 3), dtype=np.uint8)\n    \n    for i, label in enumerate(labels):\n        color_map[segmentation_mask == i] = label.color  # Set the color for the corresponding class\n    \n    return color_map\n\ndef process_images(input_folder, output_folder, model):\n    \"\"\"Processes images from the input folder and saves color maps in the output folder.\"\"\"\n    # Ensure the output folder exists\n    os.makedirs(output_folder, exist_ok=True)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Move model to the appropriate device\n    model.to(device)\n\n    # Loop through all files in the input folder\n    for filename in os.listdir(input_folder):\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # You can add more formats if needed\n            image_path = os.path.join(input_folder, filename)\n            image = Image.open(image_path)\n\n            # Convert image to numpy array\n            image_np = np.array(image)\n\n            # Apply transformations\n            transformed = val_transforms(image=image_np)\n            image_transformed = transformed['image']\n\n            # Convert to tensor and add batch dimension\n            inputs = torch.tensor(image_transformed, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device)  # Move to device\n\n            # Perform inference\n            with torch.no_grad():\n                outputs = model(inputs)\n\n            # Get the predicted class masks\n            logits = outputs.logits\n            predicted_segmentation_mask = torch.argmax(logits, dim=1)\n            predicted_segmentation_mask = predicted_segmentation_mask.squeeze().cpu().numpy()\n\n            # Resize the segmentation mask to 1920x1080\n            predicted_segmentation_mask_resized = Image.fromarray(predicted_segmentation_mask.astype(np.uint8)).resize((1920, 1080), Image.NEAREST)\n            predicted_segmentation_mask_resized = np.array(predicted_segmentation_mask_resized)\n\n            # Create a color map from the segmentation mask\n            color_map_image = create_color_map(predicted_segmentation_mask_resized, labels)\n\n            # Save the color map image with the same filename in the output folder\n            output_image_path = os.path.join(output_folder, filename)  # Keep the same filename\n            Image.fromarray(color_map_image).save(output_image_path)\n\n            print(f\"Processed {filename} -> Saved color map to {output_image_path}\")\n\n# Define the input and output folder paths\ninput_folder = r\"/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/test\"  # Update with your actual input folder\noutput_folder = r\"/kaggle/working/my_directory\"  # This is where the output will be saved\n\n# Load your model (replace this with your actual model loading code)\n  # Placeholder: replace with actual model initialization\n\n# Ensure the model is loaded correctly\nif model is not None:\n    # Process the images\n    process_images(input_folder, output_folder, model)\nelse:\n    print(\"Model is not loaded. Please load your model before processing images.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:45:24.908507Z","iopub.execute_input":"2024-10-13T03:45:24.90901Z","iopub.status.idle":"2024-10-13T03:45:48.680966Z","shell.execute_reply.started":"2024-10-13T03:45:24.90895Z","shell.execute_reply":"2024-10-13T03:45:48.679836Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nclass Label:\n    def __init__(self, name, id, csId, csTrainId, level4id, level3Id, category, level2Id, level1Id, hasInstances, ignoreInEval, color):\n        self.name = name\n        self.id = id\n        self.csId = csId\n        self.csTrainId = csTrainId\n        self.level4id = level4id\n        self.level3Id = level3Id\n        self.category = category\n        self.level2Id = level2Id\n        self.level1Id = level1Id\n        self.hasInstances = hasInstances\n        self.ignoreInEval = ignoreInEval\n        self.color = color\nlabels = [\n    Label('road', 0, 7, 0, 0, 0, 'drivable', 0, 0, False, False, (128, 64, 128)),\n    Label('parking', 1, 9, 255, 1, 1, 'drivable', 1, 0, False, False, (250, 170, 160)),\n    Label('drivable fallback', 2, 255, 255, 2, 1, 'drivable', 1, 0, False, False, (81, 0, 81)),\n    Label('sidewalk', 3, 8, 1, 3, 2, 'non-drivable', 2, 1, False, False, (244, 35, 232)),\n    Label('rail track', 4, 10, 255, 3, 3, 'non-drivable', 3, 1, False, False, (230, 150, 140)),\n    Label('non-drivable fallback', 5, 255, 9, 4, 3, 'non-drivable', 3, 1, False, False, (152, 251, 152)),\n    Label('person', 6, 24, 11, 5, 4, 'living-thing', 4, 2, True, False, (220, 20, 60)),\n    Label('animal', 7, 255, 255, 6, 4, 'living-thing', 4, 2, True, True, (246, 198, 145)),\n    Label('rider', 8, 25, 12, 7, 5, 'living-thing', 5, 2, True, False, (255, 0, 0)),\n    Label('motorcycle', 9, 32, 17, 8, 6, '2-wheeler', 6, 3, True, False, (0, 0, 230)),\n    Label('bicycle', 10, 33, 18, 9, 7, '2-wheeler', 6, 3, True, False, (119, 11, 32)),\n    Label('autorickshaw', 11, 255, 255, 10, 8, 'autorickshaw', 7, 3, True, False, (255, 204, 54)),\n    Label('car', 12, 26, 13, 11, 9, 'car', 7, 3, True, False, (0, 0, 142)),\n    Label('truck', 13, 27, 14, 12, 10, 'large-vehicle', 8, 3, True, False, (0, 0, 70)),\n    Label('bus', 14, 28, 15, 13, 11, 'large-vehicle', 8, 3, True, False, (0, 60, 100)),\n    Label('caravan', 15, 29, 255, 14, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 90)),\n    Label('trailer', 16, 30, 255, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 0, 110)),\n    Label('train', 17, 31, 16, 15, 12, 'large-vehicle', 8, 3, True, True, (0, 80, 100)),\n    Label('vehicle fallback', 18, 355, 255, 15, 12, 'large-vehicle', 8, 3, True, False, (136, 143, 153)),\n    Label('curb', 19, 255, 255, 16, 13, 'barrier', 9, 4, False, False, (220, 190, 40)),\n    Label('wall', 20, 12, 3, 17, 14, 'barrier', 9, 4, False, False, (102, 102, 156)),\n    Label('fence', 21, 13, 4, 18, 15, 'barrier', 10, 4, False, False, (190, 153, 153)),\n    Label('guard rail', 22, 14, 255, 19, 16, 'barrier', 10, 4, False, False, (180, 165, 180)),\n    Label('billboard', 23, 255, 255, 20, 17, 'structures', 11, 4, False, False, (174, 64, 67)),\n    Label('traffic sign', 24, 20, 7, 21, 18, 'structures', 11, 4, False, False, (220, 220, 0)),\n    Label('traffic light', 25, 19, 6, 22, 19, 'structures', 11, 4, False, False, (250, 170, 30)),\n    Label('pole', 26, 17, 5, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n    Label('polegroup', 27, 18, 255, 23, 20, 'structures', 12, 4, False, False, (153, 153, 153)),\n    Label('obs-str-bar-fallback', 28, 255, 255, 24, 21, 'structures', 12, 4, False, False, (169, 187, 214)),\n    Label('building', 29, 11, 2, 25, 22, 'construction', 13, 5, False, False, (70, 70, 70)),\n    Label('bridge', 30, 15, 255, 26, 23, 'construction', 13, 5, False, False, (150, 100, 100)),\n    Label('tunnel', 31, 16, 255, 26, 23, 'construction', 13, 5, False, False, (150, 120, 90)),\n    Label('vegetation', 32, 21, 8, 27, 24, 'vegetation', 14, 5, False, False, (107, 142, 35)),\n    Label('sky', 33, 23, 10, 28, 25, 'sky', 15, 6, False, False, (70, 130, 180)),\n    Label('fallback background', 34, 255, 255, 29, 25, 'object fallback', 15, 6, False, False, (169, 187, 214)),\n    Label('unlabeled', 35, 0, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n    Label('ego vehicle', 36, 1, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n    Label('rectification border', 37, 2, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n    Label('out of roi', 38, 3, 255, 255, 255, 'void', 255, 255, False, True, (0, 0, 0)),\n    Label('license plate', 39, 255, 255, 255, 255, 'vehicle', 255, 255, False, True, (0, 0, 142))\n]\ndef create_color_map(segmentation_array, labels):\n    height, width = segmentation_array.shape\n    color_image = np.zeros((height, width, 3), dtype=np.uint8)\n\n    # Apply the color based on the segmentation mask's label ids\n    for label in labels:\n        color_image[segmentation_array == label.id] = label.color\n\n    return color_image\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:45:48.683163Z","iopub.execute_input":"2024-10-13T03:45:48.68403Z","iopub.status.idle":"2024-10-13T03:45:48.716654Z","shell.execute_reply.started":"2024-10-13T03:45:48.683978Z","shell.execute_reply":"2024-10-13T03:45:48.715695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom PIL import Image\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport time\n\n# Assuming the model and image processor are already defined and loaded\n# model = ... (load your model here)\n\n# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)  # Move model to the appropriate device\n\n# Define the validation transformations\nval_transforms = A.Compose([\n    A.Resize(width=512, height=512),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ndef create_color_map(segmentation_mask, labels):\n    \"\"\"Creates a color map from the segmentation mask using provided labels.\"\"\"\n    color_map = np.zeros((segmentation_mask.shape[0], segmentation_mask.shape[1], 3), dtype=np.uint8)\n    \n    for i, label in enumerate(labels):\n        color_map[segmentation_mask == i] = label.color  # Set the color for the corresponding class\n    \n    return color_map\n\ndef calculate_iou(pred_mask, gt_mask, num_classes):\n    \"\"\"Calculates IoU for each class.\"\"\"\n    iou = []\n    for cls in range(num_classes):\n        # Calculate true positives, false positives, and false negatives\n        intersection = np.logical_and(pred_mask == cls, gt_mask == cls).sum()\n        area_pred = (pred_mask == cls).sum()\n        area_gt = (gt_mask == cls).sum()\n        \n        # Compute IoU\n        union = area_pred + area_gt - intersection\n        iou_value = intersection / union if union != 0 else 0  # Avoid division by zero\n        iou.append(iou_value)\n    \n    return iou\n\ndef convert_gt_mask(gt_mask):\n    \"\"\"Convert the ground truth mask to a single channel.\"\"\"\n    # Assuming gt_mask has shape (H, W, 4), convert to (H, W)\n    return np.argmax(gt_mask, axis=-1)  # Change this depending on how your ground truth is structured\n\ndef process_single_image(image_path, gt_path):\n    \"\"\"Processes a single image and visualizes the color map, including inference time and IoU.\"\"\"\n    image = Image.open(image_path)\n    gt_image = Image.open(gt_path)  # Load the ground truth mask\n\n    # Convert images to numpy arrays\n    image_np = np.array(image)\n    gt_mask = np.array(gt_image)\n\n    # Convert ground truth mask to single channel\n    gt_mask_single_channel = convert_gt_mask(gt_mask)\n\n    # Apply transformations\n    transformed = val_transforms(image=image_np)\n    image_transformed = transformed['image']\n\n    # Convert to tensor and add batch dimension\n    inputs = torch.tensor(image_transformed).permute(2, 0, 1).unsqueeze(0).to(device)  # Move to the same device\n\n    # Measure inference time\n    start_time = time.time()\n    \n    # Perform inference\n    with torch.no_grad():\n        outputs = model(inputs)\n\n    # Calculate inference time\n    inference_time = time.time() - start_time\n\n    # Get the predicted class masks\n    logits = outputs.logits\n    predicted_segmentation_mask = torch.argmax(logits, dim=1)\n    predicted_segmentation_mask = predicted_segmentation_mask.squeeze().cpu().numpy()\n\n    # Resize the segmentation mask to 1920x1080\n    predicted_segmentation_mask_resized = Image.fromarray(predicted_segmentation_mask.astype(np.uint8)).resize((1920, 1080), Image.NEAREST)\n    predicted_segmentation_mask_resized = np.array(predicted_segmentation_mask_resized)\n\n    # Create a color map from the segmentation mask\n    color_map_image = create_color_map(predicted_segmentation_mask_resized, labels)\n\n    # Calculate IoU using the single channel ground truth mask\n    iou_values = calculate_iou(predicted_segmentation_mask_resized, gt_mask_single_channel, len(labels))\n    \n    # Visualize the original image and the color map\n    plt.figure(figsize=(12, 6))\n\n    # Original Image\n    plt.subplot(1, 2, 1)\n    plt.title(\"Original Image\")\n    plt.imshow(image_np)\n    plt.axis(\"off\")\n\n    # Color Map\n    plt.subplot(1, 2, 2)\n    plt.title(\"Predicted Color Map\")\n    plt.imshow(color_map_image)\n    plt.axis(\"off\")\n\n    plt.show()\n\n    # Print inference time and IoU\n    print(f\"Inference Time: {inference_time:.4f} seconds\")\n    print(\"IoU Values per Class:\", iou_values)\n\n# Specify the path to your image and its corresponding ground truth\nimage_path = r\"/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/train/201/frame0029_leftImg8bit.jpg\"  # Update with your actual image path\ngt_path = r\"/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/labels/201/frame0029_gtFine_labelColors.png\"  # Update with your actual ground truth path\n\n# Process and visualize the single image\nprocess_single_image(image_path, gt_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:45:48.718191Z","iopub.execute_input":"2024-10-13T03:45:48.7186Z","iopub.status.idle":"2024-10-13T03:45:50.024513Z","shell.execute_reply.started":"2024-10-13T03:45:48.71855Z","shell.execute_reply":"2024-10-13T03:45:50.0236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_path = r\"/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/train/204/frame0066_leftImg8bit.jpg\"  # Update with your actual image path\ngt_path = r\"/kaggle/input/iitg-ai-overnight-hackathon-2024/dataset/dataset/labels/204/frame0066_gtFine_labelColors.png\"  # Update with your actual ground truth path\n\n# Process and visualize the single image\nprocess_single_image(image_path, gt_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:47:23.077665Z","iopub.execute_input":"2024-10-13T03:47:23.078526Z","iopub.status.idle":"2024-10-13T03:47:24.333799Z","shell.execute_reply.started":"2024-10-13T03:47:23.078474Z","shell.execute_reply":"2024-10-13T03:47:24.332872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\nimport IPython.display as display\n\n# Specify the folder you want to zip and download\nfolder_path = '/kaggle/working/segformer-b0-finetuned-segments-sidewalk-outputs/checkpoint-3360'  # Change this to your folder's path\nzip_file_path = '/kaggle/working/my_results.zip'  # Output ZIP file path\n\n# Create a ZIP archive from the specified folder\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n\n# Provide a download link\ndisplay.FileLink(zip_file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-13T03:45:50.026249Z","iopub.execute_input":"2024-10-13T03:45:50.026587Z","iopub.status.idle":"2024-10-13T03:45:50.035158Z","shell.execute_reply.started":"2024-10-13T03:45:50.026551Z","shell.execute_reply":"2024-10-13T03:45:50.034392Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}